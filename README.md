# ğŸµ MoodifyMusic - Emotion-Based Music Recommender ğŸ­ğŸ¶

MoodifyMusic is a real-time emotion detection system that recommends songs based on your current mood. It uses your facial and hand gestures via your webcam to predict emotions with a machine learning model and recommends matching songs from YouTube.

---

## ğŸš€ Features

- ğŸ¥ Real-time webcam-based emotion detection
- ğŸ§  Deep learning model trained on landmark data (face + hands)
- ğŸ¯ Predicts emotions like happy, sad, angry, etc.
- ğŸ”— Recommends music based on:
  - Detected emotion
  - Selected language
  - Optional singer input
- ğŸŒ Built using Streamlit + WebRTC

---

## ğŸ¬ Demo

> ğŸ“º **Watch the Live Demo Below**  
> *(Click the video or open it in YouTube)*

[![MoodifyMusic Demo](https://img.youtube.com/vi/jsJrREv9lyY/0.jpg)](https://www.youtube.com/watch?v=jsJrREv9lyY)

---

## ğŸ› ï¸ Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/your-username/MoodifyMusic.git
   cd MoodifyMusic
Install dependencies:

```bash
pip install -r requirements.txt
```
Run the app:

```bash
streamlit run music.py
```
ğŸ“‚ Project Structure
```bash
MoodifyMusic/
â”œâ”€â”€ liveEmoji        # Folder for training the model 
â”œâ”€â”€ music.py                  # Streamlit app for real-time emotion detection
â”œâ”€â”€ model.h5                # Trained Keras model
â”œâ”€â”€ labels.npy              # Emotion labels
â”œâ”€â”€ emotion.npy             # Temp file to store detected emotion
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
ğŸ“¦ Dependencies
```txt
streamlit
streamlit-webrtc
opencv-python
mediapipe
tensorflow
numpy
av
```
Install using:

```bash
pip install -r requirements.txt
```
ğŸ“¸ How It Works
Webcam captures your facial and hand movements.

Mediapipe extracts landmark positions.

Pre-trained neural network predicts your emotion.

You click â€œRecommend me songsâ€ â€” it opens a YouTube search like:

```arduino
https://www.youtube.com/results?search_query=English+happy+song+Ed+Sheeran
```
